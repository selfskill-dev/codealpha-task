# -*- coding: utf-8 -*-
"""Emotion-Recognize-From-Speech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qwovQx0Qk91Slhc6mzovJoG01UGXitDW
"""

!pip install librosa soundfile numpy keras tensorflow scikit-learn

from google.colab import files
uploaded = files.upload()

!pip install resampy

import librosa
import numpy as np

def extract_features(audio_file):
    audio, sample_rate = librosa.load(audio_file, res_type='kaiser_fast')
    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
    mfccs_scaled = np.mean(mfccs.T, axis=0)
    return mfccs_scaled

features = extract_features("voice1.wav")
print("Extracted feature shape:", features.shape)

!pip install --upgrade --force-reinstall librosa resampy numpy

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential()
model.add(Dense(256, input_shape=(40,), activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(6, activation='softmax'))  # 6 emotions

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# For demo only â€” random prediction
dummy_prediction = np.random.rand(1,6)
predicted_label = np.argmax(dummy_prediction)

emotion_labels = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful']
print("Predicted Emotion for voice1.wav:", emotion_labels[predicted_label])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import numpy as np

# Build a simple model
model = Sequential()
model.add(Dense(256, input_shape=(40,), activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(6, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train with dummy data (just to activate model for prediction)
dummy_X = np.random.rand(100, 40)
dummy_y = np.zeros((100, 6))
dummy_y[:, 2] = 1  # simulate "Happy"

model.fit(dummy_X, dummy_y, epochs=5, batch_size=8, verbose=0)

features = extract_features("voice1.wav")
features = np.expand_dims(features, axis=0)

prediction = model.predict(features)
predicted_label = np.argmax(prediction)

emotion_labels = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful']
print("Predicted Emotion for voice1.wav:", emotion_labels[predicted_label])